kind: Deployment
apiVersion: apps/v1
metadata:
  name: ollama-serve
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama-serve
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: ollama-serve
    spec:
      volumes:
        - name: ollama-storage
          persistentVolumeClaim:
            claimName: ollama-storage
        - name: model-config
          configMap:
            name: ollama-models-config
      containers:
        - resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              cpu: 2m
              memory: 5Gi
              nvidia.com/gpu: '1'
          terminationMessagePath: /dev/termination-log
          lifecycle:
            postStart:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                    while read model; do
                      echo "Pulling model: $model"
                      ollama pull $model
                    done < /app/models/models.txt; ollama list
          name: container
          env:
            - name: OLLAMA_KEEP_ALIVE
              value: '-1m'
            - name: OLLAMA_ORIGINS
              value: '*'
            - name: OLLAMA_CONTEXT_LENGTH
              value: '16384'
          ports:
            - containerPort: 11434
              protocol: TCP
          imagePullPolicy: Always
          volumeMounts:
            - name: ollama-storage
              mountPath: /.ollama
            - name: model-config
              mountPath: /app/models
          terminationMessagePolicy: File
          image: 'quay.io/redhat-ai-dev/ollama-ubi:v0.9.1'
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      securityContext: {}
      schedulerName: default-scheduler
  strategy:
    type: Recreate
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
